<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D Face Detection</title>

    <!-- Importing necessary libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>

    <style>
        body { font-family: Arial, sans-serif; text-align: center; }
        .video-container { position: relative; width: 640px; height: 480px; margin: auto; }
        video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
    </style>
</head>
<body>
    <h2>3D Face Detection</h2>
    <div class="video-container">
        <video id="video" autoplay playsinline></video>
        <canvas id="face-overlay"></canvas>
    </div>

    <script>
        let faceMesh, camera, scene, renderer, camera3D, facePoints;
        const videoWidth = 640;
        const videoHeight = 480;

        async function loadFaceMesh() {
            faceMesh = new FaceMesh({
                locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
            });

            faceMesh.setOptions({
                maxNumFaces: 5,
                refineLandmarks: true,
                minDetectionConfidence: 0.7,
                minTrackingConfidence: 0.7
            });

            faceMesh.onResults(onFaceMeshResults);
        }

        function startCamera() {
            const videoElement = document.getElementById('video');
            camera = new Camera(videoElement, {
                onFrame: async () => {
                    await faceMesh.send({ image: videoElement });
                },
                width: videoWidth,
                height: videoHeight
            });
            camera.start();
        }

        function setupThreeJS() {
            scene = new THREE.Scene();

            renderer = new THREE.WebGLRenderer({ alpha: true, canvas: document.getElementById('face-overlay') });
            renderer.setSize(videoWidth, videoHeight);

            camera3D = new THREE.PerspectiveCamera(65, videoWidth / videoHeight, 0.1, 100);
            camera3D.position.z = 2.5; // Adjusted for better face depth

            const geometry = new THREE.BufferGeometry();
            geometry.setAttribute('position', new THREE.BufferAttribute(new Float32Array(477 * 3), 3));

            facePoints = new THREE.Points(
                geometry,
                new THREE.PointsMaterial({ color: 0xff0000, size: 0.03 }) // ðŸ”´ Increased dot size for visibility
            );

            scene.add(facePoints);
        }

        function onFaceMeshResults(results) {
            if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) return;
            updateLandmarks(results.multiFaceLandmarks[0]);
        }

        function updateLandmarks(landmarks) {
            if (!facePoints || !facePoints.geometry) return;
            const positions = facePoints.geometry.attributes.position.array;

            landmarks.forEach((landmark, i) => {
                positions[i * 3] = (landmark.x - 0.5) * 4.5; // ðŸŽ¯ Adjusted scaling for perfect fit
                positions[i * 3 + 1] = -(landmark.y - 0.5) * 3.1;
                positions[i * 3 + 2] = -landmark.z * 3.2;
            });

            facePoints.geometry.attributes.position.needsUpdate = true;
            renderer.render(scene, camera3D);
        }

        async function init() {
            await loadFaceMesh();
            setupThreeJS();
            startCamera();
        }

        window.onload = init;
    </script>
</body>
</html>
